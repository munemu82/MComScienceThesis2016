function [trainedClassifier, validationAccuracy] = trainClassifier(trainingData)
% trainClassifier(trainingData)
%  returns a trained classifier and its accuracy.
%  This code recreates the classification model trained in
%  Classification Learner app.
%
%   Input:
%       trainingData: the training data of same data type as imported
%        in the app (table or matrix).
%
%   Output:
%       trainedClassifier: a struct containing the trained classifier.
%        The struct contains various fields with information about the
%        trained classifier.
%
%       trainedClassifier.predictFcn: a function to make predictions
%        on new data. It takes an input of the same form as this training
%        code (table or matrix) and returns predictions for the response.
%        If you supply a matrix, include only the predictors columns (or
%        rows).
%
%       validationAccuracy: a double containing the accuracy in
%        percent. In the app, the History list displays this
%        overall accuracy score for each model.
%
%  Use the code to train the model with new data.
%  To retrain your classifier, call the function from the command line
%  with your original data or new data as the input argument trainingData.
%
%  For example, to retrain a classifier trained with the original data set
%  T, enter:
%    [trainedClassifier, validationAccuracy] = trainClassifier(T)
%
%  To make predictions with the returned 'trainedClassifier' on new data T,
%  use
%    yfit = trainedClassifier.predictFcn(T)
%
%  To automate training the same classifier with new data, or to learn how
%  to programmatically train classifiers, examine the generated code.

% Auto-generated by MATLAB on 06-Apr-2016 06:12:05


% Extract predictors and response
% This code processes the data into the right shape for training the
% classifier.
inputTable = trainingData;
predictorNames = {'bagOfFeaturesData1', 'bagOfFeaturesData2', 'bagOfFeaturesData3', 'bagOfFeaturesData4', 'bagOfFeaturesData5', 'bagOfFeaturesData6', 'bagOfFeaturesData7', 'bagOfFeaturesData8', 'bagOfFeaturesData9', 'bagOfFeaturesData10', 'bagOfFeaturesData11', 'bagOfFeaturesData12', 'bagOfFeaturesData13', 'bagOfFeaturesData14', 'bagOfFeaturesData15', 'bagOfFeaturesData16', 'bagOfFeaturesData17', 'bagOfFeaturesData18', 'bagOfFeaturesData19', 'bagOfFeaturesData20', 'bagOfFeaturesData21', 'bagOfFeaturesData22', 'bagOfFeaturesData23', 'bagOfFeaturesData24', 'bagOfFeaturesData25', 'bagOfFeaturesData26', 'bagOfFeaturesData27', 'bagOfFeaturesData28', 'bagOfFeaturesData29', 'bagOfFeaturesData30', 'bagOfFeaturesData31', 'bagOfFeaturesData32', 'bagOfFeaturesData33', 'bagOfFeaturesData34', 'bagOfFeaturesData35', 'bagOfFeaturesData36', 'bagOfFeaturesData37', 'bagOfFeaturesData38', 'bagOfFeaturesData39', 'bagOfFeaturesData40', 'bagOfFeaturesData41', 'bagOfFeaturesData42', 'bagOfFeaturesData43', 'bagOfFeaturesData44', 'bagOfFeaturesData45', 'bagOfFeaturesData46', 'bagOfFeaturesData47', 'bagOfFeaturesData48', 'bagOfFeaturesData49', 'bagOfFeaturesData50', 'bagOfFeaturesData51', 'bagOfFeaturesData52', 'bagOfFeaturesData53', 'bagOfFeaturesData54', 'bagOfFeaturesData55', 'bagOfFeaturesData56', 'bagOfFeaturesData57', 'bagOfFeaturesData58', 'bagOfFeaturesData59', 'bagOfFeaturesData60', 'bagOfFeaturesData61', 'bagOfFeaturesData62', 'bagOfFeaturesData63', 'bagOfFeaturesData64', 'bagOfFeaturesData65', 'bagOfFeaturesData66', 'bagOfFeaturesData67', 'bagOfFeaturesData68', 'bagOfFeaturesData69', 'bagOfFeaturesData70', 'bagOfFeaturesData71', 'bagOfFeaturesData72', 'bagOfFeaturesData73', 'bagOfFeaturesData74', 'bagOfFeaturesData75', 'bagOfFeaturesData76', 'bagOfFeaturesData77', 'bagOfFeaturesData78', 'bagOfFeaturesData79', 'bagOfFeaturesData80', 'bagOfFeaturesData81', 'bagOfFeaturesData82', 'bagOfFeaturesData83', 'bagOfFeaturesData84', 'bagOfFeaturesData85', 'bagOfFeaturesData86', 'bagOfFeaturesData87', 'bagOfFeaturesData88', 'bagOfFeaturesData89', 'bagOfFeaturesData90', 'bagOfFeaturesData91', 'bagOfFeaturesData92', 'bagOfFeaturesData93', 'bagOfFeaturesData94', 'bagOfFeaturesData95', 'bagOfFeaturesData96', 'bagOfFeaturesData97', 'bagOfFeaturesData98', 'bagOfFeaturesData99', 'bagOfFeaturesData100', 'bagOfFeaturesData101', 'bagOfFeaturesData102', 'bagOfFeaturesData103', 'bagOfFeaturesData104', 'bagOfFeaturesData105', 'bagOfFeaturesData106', 'bagOfFeaturesData107', 'bagOfFeaturesData108', 'bagOfFeaturesData109', 'bagOfFeaturesData110', 'bagOfFeaturesData111', 'bagOfFeaturesData112', 'bagOfFeaturesData113', 'bagOfFeaturesData114', 'bagOfFeaturesData115', 'bagOfFeaturesData116', 'bagOfFeaturesData117', 'bagOfFeaturesData118', 'bagOfFeaturesData119', 'bagOfFeaturesData120', 'bagOfFeaturesData121', 'bagOfFeaturesData122', 'bagOfFeaturesData123', 'bagOfFeaturesData124', 'bagOfFeaturesData125', 'bagOfFeaturesData126', 'bagOfFeaturesData127', 'bagOfFeaturesData128', 'bagOfFeaturesData129', 'bagOfFeaturesData130', 'bagOfFeaturesData131', 'bagOfFeaturesData132', 'bagOfFeaturesData133', 'bagOfFeaturesData134', 'bagOfFeaturesData135', 'bagOfFeaturesData136', 'bagOfFeaturesData137', 'bagOfFeaturesData138', 'bagOfFeaturesData139', 'bagOfFeaturesData140', 'bagOfFeaturesData141', 'bagOfFeaturesData142', 'bagOfFeaturesData143', 'bagOfFeaturesData144', 'bagOfFeaturesData145', 'bagOfFeaturesData146', 'bagOfFeaturesData147', 'bagOfFeaturesData148', 'bagOfFeaturesData149', 'bagOfFeaturesData150'};
predictors = inputTable(:, predictorNames);
response = inputTable.class;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Train a classifier
% This code specifies all the classifier options and trains the classifier.
classificationSVM = fitcsvm(...
    predictors, ...
    response, ...
    'KernelFunction', 'polynomial', ...
    'PolynomialOrder', 2, ...
    'KernelScale', 'auto', ...
    'BoxConstraint', 1, ...
    'Standardize', true, ...
    'ClassNames', categorical({'Kangaroo'; 'NotKangaroo'}));

% Create the result struct with predict function
predictorExtractionFcn = @(t) t(:, predictorNames);
svmPredictFcn = @(x) predict(classificationSVM, x);
trainedClassifier.predictFcn = @(x) svmPredictFcn(predictorExtractionFcn(x));

% Add additional fields to the result struct
trainedClassifier.RequiredVariables = {'bagOfFeaturesData1', 'bagOfFeaturesData2', 'bagOfFeaturesData3', 'bagOfFeaturesData4', 'bagOfFeaturesData5', 'bagOfFeaturesData6', 'bagOfFeaturesData7', 'bagOfFeaturesData8', 'bagOfFeaturesData9', 'bagOfFeaturesData10', 'bagOfFeaturesData11', 'bagOfFeaturesData12', 'bagOfFeaturesData13', 'bagOfFeaturesData14', 'bagOfFeaturesData15', 'bagOfFeaturesData16', 'bagOfFeaturesData17', 'bagOfFeaturesData18', 'bagOfFeaturesData19', 'bagOfFeaturesData20', 'bagOfFeaturesData21', 'bagOfFeaturesData22', 'bagOfFeaturesData23', 'bagOfFeaturesData24', 'bagOfFeaturesData25', 'bagOfFeaturesData26', 'bagOfFeaturesData27', 'bagOfFeaturesData28', 'bagOfFeaturesData29', 'bagOfFeaturesData30', 'bagOfFeaturesData31', 'bagOfFeaturesData32', 'bagOfFeaturesData33', 'bagOfFeaturesData34', 'bagOfFeaturesData35', 'bagOfFeaturesData36', 'bagOfFeaturesData37', 'bagOfFeaturesData38', 'bagOfFeaturesData39', 'bagOfFeaturesData40', 'bagOfFeaturesData41', 'bagOfFeaturesData42', 'bagOfFeaturesData43', 'bagOfFeaturesData44', 'bagOfFeaturesData45', 'bagOfFeaturesData46', 'bagOfFeaturesData47', 'bagOfFeaturesData48', 'bagOfFeaturesData49', 'bagOfFeaturesData50', 'bagOfFeaturesData51', 'bagOfFeaturesData52', 'bagOfFeaturesData53', 'bagOfFeaturesData54', 'bagOfFeaturesData55', 'bagOfFeaturesData56', 'bagOfFeaturesData57', 'bagOfFeaturesData58', 'bagOfFeaturesData59', 'bagOfFeaturesData60', 'bagOfFeaturesData61', 'bagOfFeaturesData62', 'bagOfFeaturesData63', 'bagOfFeaturesData64', 'bagOfFeaturesData65', 'bagOfFeaturesData66', 'bagOfFeaturesData67', 'bagOfFeaturesData68', 'bagOfFeaturesData69', 'bagOfFeaturesData70', 'bagOfFeaturesData71', 'bagOfFeaturesData72', 'bagOfFeaturesData73', 'bagOfFeaturesData74', 'bagOfFeaturesData75', 'bagOfFeaturesData76', 'bagOfFeaturesData77', 'bagOfFeaturesData78', 'bagOfFeaturesData79', 'bagOfFeaturesData80', 'bagOfFeaturesData81', 'bagOfFeaturesData82', 'bagOfFeaturesData83', 'bagOfFeaturesData84', 'bagOfFeaturesData85', 'bagOfFeaturesData86', 'bagOfFeaturesData87', 'bagOfFeaturesData88', 'bagOfFeaturesData89', 'bagOfFeaturesData90', 'bagOfFeaturesData91', 'bagOfFeaturesData92', 'bagOfFeaturesData93', 'bagOfFeaturesData94', 'bagOfFeaturesData95', 'bagOfFeaturesData96', 'bagOfFeaturesData97', 'bagOfFeaturesData98', 'bagOfFeaturesData99', 'bagOfFeaturesData100', 'bagOfFeaturesData101', 'bagOfFeaturesData102', 'bagOfFeaturesData103', 'bagOfFeaturesData104', 'bagOfFeaturesData105', 'bagOfFeaturesData106', 'bagOfFeaturesData107', 'bagOfFeaturesData108', 'bagOfFeaturesData109', 'bagOfFeaturesData110', 'bagOfFeaturesData111', 'bagOfFeaturesData112', 'bagOfFeaturesData113', 'bagOfFeaturesData114', 'bagOfFeaturesData115', 'bagOfFeaturesData116', 'bagOfFeaturesData117', 'bagOfFeaturesData118', 'bagOfFeaturesData119', 'bagOfFeaturesData120', 'bagOfFeaturesData121', 'bagOfFeaturesData122', 'bagOfFeaturesData123', 'bagOfFeaturesData124', 'bagOfFeaturesData125', 'bagOfFeaturesData126', 'bagOfFeaturesData127', 'bagOfFeaturesData128', 'bagOfFeaturesData129', 'bagOfFeaturesData130', 'bagOfFeaturesData131', 'bagOfFeaturesData132', 'bagOfFeaturesData133', 'bagOfFeaturesData134', 'bagOfFeaturesData135', 'bagOfFeaturesData136', 'bagOfFeaturesData137', 'bagOfFeaturesData138', 'bagOfFeaturesData139', 'bagOfFeaturesData140', 'bagOfFeaturesData141', 'bagOfFeaturesData142', 'bagOfFeaturesData143', 'bagOfFeaturesData144', 'bagOfFeaturesData145', 'bagOfFeaturesData146', 'bagOfFeaturesData147', 'bagOfFeaturesData148', 'bagOfFeaturesData149', 'bagOfFeaturesData150'};
trainedClassifier.ClassificationSVM = classificationSVM;
trainedClassifier.About = 'This struct is a trained classifier exported from Classification Learner R2016a.';
trainedClassifier.HowToPredict = sprintf('To make predictions on a new table, T, use: \n  yfit = c.predictFcn(T) \nreplacing ''c'' with the name of the variable that is this struct, e.g. ''trainedClassifier''. \n \nThe table, T, must contain the variables returned by: \n  c.RequiredVariables \nVariable formats (e.g. matrix/vector, datatype) must match the original training data. \nAdditional variables are ignored. \n \nFor more information, see <a href="matlab:helpview(fullfile(docroot, ''stats'', ''stats.map''), ''appclassification_exportmodeltoworkspace'')">How to predict using an exported model</a>.');

% Extract predictors and response
% This code processes the data into the right shape for training the
% classifier.
inputTable = trainingData;
predictorNames = {'bagOfFeaturesData1', 'bagOfFeaturesData2', 'bagOfFeaturesData3', 'bagOfFeaturesData4', 'bagOfFeaturesData5', 'bagOfFeaturesData6', 'bagOfFeaturesData7', 'bagOfFeaturesData8', 'bagOfFeaturesData9', 'bagOfFeaturesData10', 'bagOfFeaturesData11', 'bagOfFeaturesData12', 'bagOfFeaturesData13', 'bagOfFeaturesData14', 'bagOfFeaturesData15', 'bagOfFeaturesData16', 'bagOfFeaturesData17', 'bagOfFeaturesData18', 'bagOfFeaturesData19', 'bagOfFeaturesData20', 'bagOfFeaturesData21', 'bagOfFeaturesData22', 'bagOfFeaturesData23', 'bagOfFeaturesData24', 'bagOfFeaturesData25', 'bagOfFeaturesData26', 'bagOfFeaturesData27', 'bagOfFeaturesData28', 'bagOfFeaturesData29', 'bagOfFeaturesData30', 'bagOfFeaturesData31', 'bagOfFeaturesData32', 'bagOfFeaturesData33', 'bagOfFeaturesData34', 'bagOfFeaturesData35', 'bagOfFeaturesData36', 'bagOfFeaturesData37', 'bagOfFeaturesData38', 'bagOfFeaturesData39', 'bagOfFeaturesData40', 'bagOfFeaturesData41', 'bagOfFeaturesData42', 'bagOfFeaturesData43', 'bagOfFeaturesData44', 'bagOfFeaturesData45', 'bagOfFeaturesData46', 'bagOfFeaturesData47', 'bagOfFeaturesData48', 'bagOfFeaturesData49', 'bagOfFeaturesData50', 'bagOfFeaturesData51', 'bagOfFeaturesData52', 'bagOfFeaturesData53', 'bagOfFeaturesData54', 'bagOfFeaturesData55', 'bagOfFeaturesData56', 'bagOfFeaturesData57', 'bagOfFeaturesData58', 'bagOfFeaturesData59', 'bagOfFeaturesData60', 'bagOfFeaturesData61', 'bagOfFeaturesData62', 'bagOfFeaturesData63', 'bagOfFeaturesData64', 'bagOfFeaturesData65', 'bagOfFeaturesData66', 'bagOfFeaturesData67', 'bagOfFeaturesData68', 'bagOfFeaturesData69', 'bagOfFeaturesData70', 'bagOfFeaturesData71', 'bagOfFeaturesData72', 'bagOfFeaturesData73', 'bagOfFeaturesData74', 'bagOfFeaturesData75', 'bagOfFeaturesData76', 'bagOfFeaturesData77', 'bagOfFeaturesData78', 'bagOfFeaturesData79', 'bagOfFeaturesData80', 'bagOfFeaturesData81', 'bagOfFeaturesData82', 'bagOfFeaturesData83', 'bagOfFeaturesData84', 'bagOfFeaturesData85', 'bagOfFeaturesData86', 'bagOfFeaturesData87', 'bagOfFeaturesData88', 'bagOfFeaturesData89', 'bagOfFeaturesData90', 'bagOfFeaturesData91', 'bagOfFeaturesData92', 'bagOfFeaturesData93', 'bagOfFeaturesData94', 'bagOfFeaturesData95', 'bagOfFeaturesData96', 'bagOfFeaturesData97', 'bagOfFeaturesData98', 'bagOfFeaturesData99', 'bagOfFeaturesData100', 'bagOfFeaturesData101', 'bagOfFeaturesData102', 'bagOfFeaturesData103', 'bagOfFeaturesData104', 'bagOfFeaturesData105', 'bagOfFeaturesData106', 'bagOfFeaturesData107', 'bagOfFeaturesData108', 'bagOfFeaturesData109', 'bagOfFeaturesData110', 'bagOfFeaturesData111', 'bagOfFeaturesData112', 'bagOfFeaturesData113', 'bagOfFeaturesData114', 'bagOfFeaturesData115', 'bagOfFeaturesData116', 'bagOfFeaturesData117', 'bagOfFeaturesData118', 'bagOfFeaturesData119', 'bagOfFeaturesData120', 'bagOfFeaturesData121', 'bagOfFeaturesData122', 'bagOfFeaturesData123', 'bagOfFeaturesData124', 'bagOfFeaturesData125', 'bagOfFeaturesData126', 'bagOfFeaturesData127', 'bagOfFeaturesData128', 'bagOfFeaturesData129', 'bagOfFeaturesData130', 'bagOfFeaturesData131', 'bagOfFeaturesData132', 'bagOfFeaturesData133', 'bagOfFeaturesData134', 'bagOfFeaturesData135', 'bagOfFeaturesData136', 'bagOfFeaturesData137', 'bagOfFeaturesData138', 'bagOfFeaturesData139', 'bagOfFeaturesData140', 'bagOfFeaturesData141', 'bagOfFeaturesData142', 'bagOfFeaturesData143', 'bagOfFeaturesData144', 'bagOfFeaturesData145', 'bagOfFeaturesData146', 'bagOfFeaturesData147', 'bagOfFeaturesData148', 'bagOfFeaturesData149', 'bagOfFeaturesData150'};
predictors = inputTable(:, predictorNames);
response = inputTable.class;
isCategoricalPredictor = [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false];

% Set up holdout validation
cvp = cvpartition(response, 'Holdout', 0.1);
trainingPredictors = predictors(cvp.training,:);
trainingResponse = response(cvp.training,:);
trainingIsCategoricalPredictor = isCategoricalPredictor;

% Train a classifier
% This code specifies all the classifier options and trains the classifier.
classificationSVM = fitcsvm(...
    trainingPredictors, ...
    trainingResponse, ...
    'KernelFunction', 'polynomial', ...
    'PolynomialOrder', 2, ...
    'KernelScale', 'auto', ...
    'BoxConstraint', 1, ...
    'Standardize', true, ...
    'ClassNames', categorical({'Kangaroo'; 'NotKangaroo'}));

% Create the result struct with predict function
svmPredictFcn = @(x) predict(classificationSVM, x);
validationPredictFcn = @(x) svmPredictFcn(x);

% Add additional fields to the result struct

% Compute validation accuracy
validationPredictors = predictors(cvp.test,:);
validationResponse = response(cvp.test,:);

[validationPredictions, validationScores] = validationPredictFcn(validationPredictors);
correctPredictions = (validationPredictions == validationResponse);
validationAccuracy = sum(correctPredictions)/length(correctPredictions);
